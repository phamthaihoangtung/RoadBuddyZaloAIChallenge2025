{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e421af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add src to path to import modules\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append('src')\n",
    "\n",
    "import numpy as np\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"UNSLOTH_STABLE_DOWNLOADS\"] = \"1\"\n",
    "\n",
    "import json\n",
    "from typing import Callable, Dict, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import unsloth\n",
    "\n",
    "from utils.utils import (\n",
    "    load_config,\n",
    "    normalize_quantization,\n",
    "    save_submission_csv,\n",
    "    load_json_data\n",
    ")\n",
    "from utils.postprocessing import post_process_qwen3vl_output\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.utils import load_model\n",
    "from data import build_user_content\n",
    "from infer import predict_answer\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# --- Main inference logic ---\n",
    "config_path = \"configs/config_unsloth_infer.yaml\"\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(config_path)\n",
    "model_name = config.get(\"model_name\", \"DAMO-NLP-SG/VideoLLaMA3-7B\")\n",
    "infer_data_path = config.get(\"infer_data_path\", \n",
    "                             os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(''))), \"data/public_test/public_test.json\"))\n",
    "output_path = config.get(\"output_path\", None)\n",
    "attn_implementation = config.get(\"attn_implementation\", \"flash_attention_2\")\n",
    "quantization = normalize_quantization(config)\n",
    "use_unsloth = config.get(\"use_unsloth\", False)\n",
    "signs_dir = config.get(\"signs_dir\", \"data/traffic_signs\")\n",
    "\n",
    "# Load traffic signs\n",
    "from glob import glob\n",
    "signs = glob(os.path.join(signs_dir, \"*\")) if signs_dir and os.path.exists(signs_dir) else None\n",
    "\n",
    "\n",
    "# Get tokens from environment\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model, processor, tokenizer = load_model(\n",
    "    model_name=model_name,\n",
    "    attn_implementation=attn_implementation,\n",
    "    quantization=quantization,\n",
    "    use_unsloth=use_unsloth,\n",
    "    hf_token=hf_token,\n",
    "    inference_mode=True,\n",
    ")\n",
    "\n",
    "\n",
    "# # Run inference\n",
    "# print(\"Running inference...\")\n",
    "# results = run_inference(\n",
    "#     model, \n",
    "#     processor, \n",
    "#     tokenizer, \n",
    "#     test_data, \n",
    "#     model_name, \n",
    "#     use_unsloth,\n",
    "#     signs=signs\n",
    "# )\n",
    "\n",
    "# # Save results\n",
    "# print(\"Saving results...\")\n",
    "# save_submission_csv(results, infer_data_path, output_path)\n",
    "\n",
    "# print(\"Inference complete. Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data = load_json_data(infer_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162e318",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = test_data['data'][0]  # Example for single item inference\n",
    "\n",
    "messages = build_user_content(\n",
    "        item[\"video_path\"], \n",
    "        item[\"question\"], \n",
    "        item[\"choices\"], \n",
    "        use_unsloth=use_unsloth,\n",
    "        signs=signs\n",
    "    )\n",
    "response = predict_answer(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        tokenizer=tokenizer,\n",
    "        messages=messages,\n",
    "        model_name=model_name,\n",
    "        use_unsloth=use_unsloth,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97092956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# DEBUG = True\n",
    "# if DEBUG:\n",
    "#     test_data['data'] = test_data['data'][:5]  # Limit to first 5 items for debugging\n",
    "\n",
    "results = []\n",
    "\n",
    "for item in tqdm(test_data[\"data\"]):\n",
    "    t1 = time()\n",
    "    messages = build_user_content(\n",
    "        item[\"video_path\"], \n",
    "        item[\"question\"], \n",
    "        item[\"choices\"], \n",
    "        use_unsloth,\n",
    "        signs=signs\n",
    "    )\n",
    "    response = predict_answer(\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            tokenizer=tokenizer,\n",
    "            messages=messages,\n",
    "            model_name=model_name,\n",
    "            use_unsloth=use_unsloth,\n",
    "        )\n",
    "    \n",
    "    response = post_process_qwen3vl_output(response)\n",
    "    t2 = time()\n",
    "    predicted_time = int(t2*1000 - t1*1000)\n",
    "    results.append({\"id\": item.get(\"id\", \"\"), \"answer\": response, 'time (millisecond)': predicted_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "result_df.to_csv(\"time_submission.csv\", index=False)\n",
    "result_df[['id', 'answer']].to_csv(\"jupyter_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131f818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "road-buddy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
