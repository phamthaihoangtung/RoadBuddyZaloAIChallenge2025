# Example configuration for training
batch_size: 1
epochs: 2
learning_rate: 2e-4
# model_name: "models/qwen3vl-8b-higher-rank"
model_name: "models/unsloth_lora"
# output_model_path: "models/qwen3vl-8b-higher-rank"
# model_name: "unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit"
# model_name: "models/unsloth_lora"
train_data_path: "data/train/train.json"
infer_data_path: "data/public_test/public_test.json"
output_path: "outputs/qwen3vl-2b"

use_unsloth: true

use_wandb: false
wandb_run_name: ""

# Quantization: Unsloth currently honors only 4bit via load_in_4bit
quantization:
  enabled: true
  mode: "4bit"

lora:
  r: 64 # The larger, the higher the accuracy, but might overfit
  lora_alpha: 64  # Recommended alpha == r at least