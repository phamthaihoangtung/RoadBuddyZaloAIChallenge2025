# Example configuration for training
batch_size: 1
epochs: 2
learning_rate: 2e-4
# model_name: "models/qwen3vl-8b-higher-rank"
model_name: "models/qwen3vl-8b-signs-responses-only"
# output_model_path: "models/qwen3vl-8b-signs-responses-only"
# model_name: "unsloth/Qwen3-VL-8B-Instruct"
train_data_path: "data/train/train.json"
infer_data_path: "data/public_test/public_test.json"
output_path: "outputs/qwen3vl-8b-signs-responses-only"

use_unsloth: true

use_wandb: false
wandb_run_name: ""

# Quantization: Unsloth currently honors only 4bit via load_in_4bit
quantization:
  enabled: true
  mode: "4bit"

# lora:
#   r: 64 # The larger, the higher the accuracy, but might overfit
#   lora_alpha: 64  # Recommended alpha == r at least