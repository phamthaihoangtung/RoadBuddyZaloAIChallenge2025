# Example configuration for training
batch_size: 32
epochs: 10
learning_rate: 0.001
model_name: "placeholder"
infer_data_path: "data/public_test/public_test.json"
# attn_implementation: "flash_attention_2"  # Options: "flash_attention_2" or "sdpa"
# Set to true to use Unsloth FastVisionModel instead of standard HuggingFace
use_unsloth: false

# Quantization settings
quantization:
  enabled: false
  mode: "8bit"

# For Unsloth, recommended model names:
# - "unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit"
# - "unsloth/Qwen2.5-VL-7B-Instruct"

# When use_unsloth is true, use_8bit_quantization controls load_in_4bit for Unsloth
# (Unsloth uses 4-bit by default when quantization is enabled)

# Example SmolVLM2 configuration (uncomment to use)
# model_name: "HuggingFaceTB/SmolVLM2-2.2B-Video-Instruct"  # or SmolVLM2-500M-Video-Instruct
# model_name: "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
# attn_implementation: "flash_attention_2"
# use_8bit_quantization: false  # Optional: set to true for lower memory usage
